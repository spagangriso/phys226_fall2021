{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Transformation methods (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this question you will:\n",
    "\n",
    "- understand how probability distributions transform under a change of variables\n",
    "- learn how to sample from arbitrary distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions",
     "problems with bayes",
     "solutions with bayes"
    ]
   },
   "source": [
    "Suppose $x$ is a random variable with probability density function (PDF) $f(x)$, such that the probability of finding $x$ in any interval $[a,b]$ is given by\n",
    "$$\n",
    "P( a \\le x \\le b) = \\int\\limits_{a}^{b} \\! f(x) \\, dx.\n",
    "$$\n",
    "Any function $y = \\phi(x)$ of $x$ is also a random variable, which must inherit its statistical properties from $x$, but filtered through the functional transformation.\n",
    "\n",
    "Specifically, suppose that $\\phi(x)$ is a smooth and invertible function, so that $x$ uniquely determines the corresponding value of $y$, or vice versa, and small changes to $x$ lead smoothly to small changes in the value of $y$, or conversely.\n",
    "\n",
    "Then the probability density function, say $g(y)$, for $y$ may be related to $f(x)$ by demanding that probabilities for $x$ falling in a small interval, and $y$ falling in the corresponding interval, are in fact equal:\n",
    "$$\n",
    "g(y)\\,  | dy | = f(x) \\, | dx | ,\n",
    "$$\n",
    "or\n",
    "$$\n",
    "f(x) = g(y) \\, \\left|\\tfrac{dy}{dx} \\right| = g\\bigl( \\phi(x) \\bigr)  \\left|\\tfrac{d\\phi(x)}{dx} \\right|,\n",
    "$$\n",
    "where $y = \\phi(x)$ or $x = \\phi^{-1}(y)$.\n",
    "\n",
    "Equivalently, we can relate the corresponding cumulative distribution functions (CDFs), defined by\n",
    "$$\n",
    "F(u) = P( x \\le u ) = \\int\\limits_{-\\infty}^{u} \\!  f(x) \\, dx,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "G(\\xi) = P( y \\le \\xi) = \\int\\limits_{-\\infty}^{\\xi} \\!  g(y) \\, dy.\n",
    "$$\n",
    "Since $y = \\phi(x)$ is invertible, it must be strictly monotonic.  Assuming $\\phi(x)$ is increasing, the CDFs must be related by\n",
    "$$\n",
    "G(y) = F\\bigl(  \\phi^{-1}(y)  \\bigr) = F(x),\n",
    "$$\n",
    "or\n",
    "$$\n",
    "F(x) = G\\bigl( \\phi(x) \\bigr) = G(y).\n",
    "$$\n",
    "If instead $\\phi(x)$ is decreasing rather than increasing, then\n",
    "$$\n",
    "G(y) = 1-F\\bigl(  \\phi^{-1}(y) \\bigr) = 1 - F(x),\n",
    "$$\n",
    "or\n",
    "$$\n",
    "F(x) = 1-G\\bigl( \\phi(x) \\bigr) = 1 - G(y).\n",
    "$$\n",
    "\n",
    "So, in order to generate random deviates $x$ with specified probability distribution $f(x)$, we can sometimes start with a random variable $x$ with distribution $x$, and then use an appropriate change of variable $x = \\phi^{-1}(y)$.\n",
    "\n",
    "<div style=\"width: 500px;margin: auto\" align=\"center\">\n",
    "    <img src=\"transformation.png\">\n",
    "    Illustration of the transformation method, generating pseudo-random deviates $x$ drawn from $f(x) = \\tfrac{d}{dx} F(x)$ starting with uniform deviates $y$ on $[0,1)$.\n",
    "</div>\n",
    "\n",
    "In particular, suppose $y$ is uniformly distributed over the interval $[0,1)$, so the corresponding normalized pdf is \n",
    "$$\n",
    "g(y) = \\begin{cases}\n",
    "1 &\\text{ if } 0 \\le y < 1 \\\\\n",
    "0 &\\text{ otherwise}\n",
    "\\end{cases},\n",
    "$$\n",
    "and the associated CDF is \n",
    "$$\n",
    "G(y) = \\begin{cases}\n",
    "0 &\\text{ if } y \\le 0     \\\\\n",
    "y &\\text{ if } 0 < y < 1 \\\\\n",
    "1 &\\text{ if } y \\ge 1 \n",
    "\\end{cases}.\n",
    "$$\n",
    "Many pseudo-random number generators output just such uniform deviates (to a good approximation).\n",
    "\n",
    "To instead generate random deviates with the CDF $F(x)$, we can select $y$ uniformly at random in $[0,1)$, and then calculate $x$ using the inverse CDF, $x = F^{-1}(y)$.  That is to say, we draw a cumulative probability $y$ for $x$ uniformly at random, then find that value of $x$ corresponding to this probability.  Since the CDF $F(x)$ must be real, nonnegative, and non-decreasing, and strictly increasing when, by assumption, $y = \\phi(x)$ is strictly monotonic, the function $G(y)$ will be invertible in principle.  The challenge in practice is usually to find an efficient way to calculate the inverse numerically.  \n",
    "\n",
    "To see why this simple _transformation_ trick works, refer to the figure above, and  notice that, by construction, $g(y) = 1$ over the relevant range and $y = F(x)$, so \n",
    "$g(y) \\,\\left| \\tfrac{dy}{dx} \\right|= 1\\cdot \\left| f(x) \\right| = f(x)$, as desired.\n",
    "\n",
    "Equivalently, we can think directly about the corresponding CDFs. By construction, $G(y) = y = F(x)$ under this assignment, which is just what we want if e presume a monotonically increasing functional relation $y = \\phi(x)$.  If instead $\\phi(x)$ were decreasing, strictly speaking we should calculate $F^{-1}(1-y)$, but $y$ and $(1-y)$ are governed by the same uniform distribution, so we can actually use $x = F^{-1}(y)$ in either case.\n",
    "\n",
    "This transformation approach can be extended to handle multi-dimensional variates, and even non-invertible mappings, provided that we can sum over all branches that get us to a given interval in $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions",
     "problems with bayes",
     "solutions with bayes"
    ]
   },
   "source": [
    "Suppose we want to simulate an experiment where we monitor collisions of proteins in some solution, in which individual collisions can be detected by, say, fluorescence effects.\n",
    "\n",
    "The times between collisions are unpredictable from macroscopic information, and if memory-less, will be described probabilistically by some exponential distribution of the form:\n",
    "$$\n",
    "f(t)  = \\frac{e^{-t/\\tau}}{\\tau},\n",
    "$$\n",
    "where $t$ (satisfying $0 \\le t < \\infty$) is the time since the start of the current observation window (usually the time since the last collision), and $\\tau$ is the mean time between collisions.\n",
    "\n",
    "For simplicity, ignore here any false positive or false negative events, supposing collisions can be reliably and unambiguously detected, and suppose that the collision times, though stochastic, can be observed and recorded with negligible measurement error.   (More realistic assumptions regarding detector efficiency and performance could be added later, along with effects arising from additional uncertainty about, say, the number of molecules in the system, or the recorded times of collisions).\n",
    "\n",
    "Find a transformation rule which, starting with uniform deviates on $[0,1)$, generates exponentially distributed deviates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the method above, write a short program to generate random numbers that follows the above distribution. Draw 1000 random numbers that follow such a distribution, fill an histogram with the x-axis being the number and the y-axis being the number of times a number within the bin boundary has been drawn. Compare the histogram with the expected analytical probability density function expected.\n",
    "\n",
    "Hints: \n",
    "- See ROOT.TRandom.Rndm() or numpy.rand()\n",
    "- It is always a good practice to set a random seed to make results reproducible (ROOT.TRandom.SetSeed(...) and numpy.random.seed(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 Rejection Method (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this question you will:\n",
    "\n",
    "- sample random numbers that distribute according to arbitrary probability density functions (distributions)\n",
    "- estimate the efficiencies of various sampling methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "*Rejection methods* provide another simple but flexible family of techniques to generate continuous random variates sampled from a target probability density function.  The advantage is that it does not require to analytically invert the desired function. The price of simplicity is that on average it might require drawing multiple uniform variates to generate one sample from the target distribution.\n",
    "\n",
    "In the simplest, one-dimensional version, suppose we seek pseudo-random variates sampled from a target PDF $p(x)$, while having (i) access to uniform variates on $[0,1)$, as well as (ii) the ability to draw variates from a \"comparison\" PDF $f(x)$ that, upon suitable re-scaling,  provides an upper bound on $p(x)$, in the sense that $0 \\le p(x) \\le c f(x)$ for all relevant $x$,  for some constant multiplier $c$ satisfying $c \\ge 1$.\n",
    "\n",
    "To generate a trial variate, first we randomly sample a variate $x$ from $f(x)$ (possibly using a transformation or other method, as described).  Then we independently draw a uniform variate $u$ in the range $0 \\le u < 1$, and calculate $y = u c f(x)$.  If $y < p(x)$, then we accept and output $x$ as a sample.  Otherwise, the trial value $x$ is rejected, and this process is repeated (using independent draws) until an acceptable $x$ is eventually obtained.\n",
    "\n",
    "<div style=\"width: 500px;margin: auto\" align=\"center\">\n",
    "    <img src=\"rejection.png\">\n",
    "    Illustration of the rejection method, generating random deviates $x$ drawn from $p(x)$ starting with random deviates $y$ drawn from $f(y)$.\n",
    "</div>\n",
    "\n",
    "Refer to the figure for a a geometric picture.  What is happening is that points are in effect being sampled *uniformly* in that part of the $(x,y)$ plane lying below the curve $p(x)$, by first uniformly sampling in that part of the $(x,y)$ plane below the $c f(x)$ curve, then only keeping the points that also lie below $p(x)$.  The corresponding $x$ coordinates will then be distributed according to the target density $p(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Show that the overall probability of acceptance of a trial variate is $\\tfrac{1}{c}$, and that the average number of trial variates required per accepted variate is $c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Show that the accepted variates will indeed be sampled faithfully from the probability density $p(x)$.  HINT: one way is to use Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "As an example, consider a target density given by a beta distribution of the form\n",
    "$$\n",
    "p(x) = \n",
    "\\begin{cases}\n",
    "30\\, x^2(1-x)^2 & \\text{ if } 0 \\le x \\le 1 \\\\\n",
    "0 & \\text{ otherwise}\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "Using a rejection method based on a uniform bounding distribution $f(x) = 1$ over $0 \\le x < 1$,  draw $n = 10^6$ variates, based on an optimal value of $c$. Plot a histogram of the number of trial variates per accepted variate, comparing to theoretical expectations. What is the average processor time required to generate each sample using this method? What is the average rejection rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Repeat the above question, but this time based on a triangular bounding distribution $f(x) \\propto \\min[x,1-x]$ over $0 \\le x \\le 1$, draw $n = 10^6$ variates, hopefully based on an optimal choice of $c$. (HINT:  you can draw variates from the triangular distribution by using a transformation method with piecewise mapping, or by considering the sum of two uniform variates.  For fun, you may want to explore both to see which is faster---the former requires an if-then test, which slows down modern computers, while the latter requires drawing two deviates).\n",
    "\n",
    "Is the improved constant $c$ in this context worth the extra effort required to generate triangular rather than uniform trial deviates?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Optimization of exposure time for counting experiments (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this question you will:\n",
    "\n",
    "- familiarize with basic counting experiment setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a counting experiment with a signal (S) and background (B) component.\n",
    "\n",
    "Let's assume our experiment can be setup in a way to either measure background-only or the sum of signal and background. One example might be counting the activity of a radioactive source using a suitable detector that, however, will also register some counts due to natural radiation; in this situation we can switch from S+B to B-only data-taking inserting or taking out the radioactive source being tested.\n",
    "\n",
    "In our setup we aim to take data for a total time $T$. We need to split this time in the B-only configuration ($T_B$) and the S+B configuration ($T_{S+B}$), such that $T_B+T_{S+B} = T$.\n",
    "\n",
    "Let's also assume we have an initial estimate of the rate at which the signal ($r_S$) and background ($r_B$) process occur that we can use for the optimization calculation, such that in a given time $t$ we expect about $N_S \\approx r_S\\cdot t$ signal events (if the radioactive source is in place) and $N_B \\approx r_B\\cdot t$ background events.\n",
    "\n",
    "You can safely assume that the total time $T$ is much larger than $1/r_{S, B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down how you can estimate the rate of signal events $r_S$ from the two measurements $N_{S+B}$ and $N_B$ of the number of events counted in the $S+B$ and $B$ configurations, where each has taken data (counted) respectively for a time $T_{S+B}$ and $T_B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find what is the optimial split between the time used to take data in the two configuration to minimize the final uncertainty on the measured rate of signal events $r_S$ and satisfying the constraint of total available time $T = T_{S+B} + T_{B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Fitting a Signal in the Presence of Background (NOT FOR THIS WEEK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this question you will:\n",
    "\n",
    "- Gain experience in performing $\\chi^2$ fits to histogrammed data; explore how the statistical significance of a signal depends on the number of signal events and the signal-to-background ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Physicists often fit for signals in the presence of background. Compared to the previous exercise, we now explore the situation where it is not possible to take data separately in background-only mode. In such cases, the significance of the measured signal depends not only on the number of signal events but also on the amount of background *and* our ability to statistically separate the two.  \n",
    "\n",
    "In this problem, we will explore fitting signal and background for a very simple case:  The signal is a Gaussian peak centered at $x=10$ with a width $\\sigma=1$ and the background is uniformly distributed between $x=0$ and $x=20$.  \n",
    "\n",
    "The code below generates fake data, allowing you to change both the number of events in the signal and the ratio of signal-to-background.  To make sure that our definition of background does not depend on the fit range, the code below defines the signal-to-background ratio as the ratio number of signal events to the number of background events in a $\\pm 2 \\sigma$ window around the signal peak.  Here is the code you will use to generate the fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "outputs": [],
   "source": [
    "# Write import math\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def makeData( SignalToBackground, nSigEvents, seed=12345):\n",
    "    \"\"\"Generates a dataset consisting of nEvents some of which are signal (a Gaussian with a width of 1 centered\n",
    "    at x=5) and the rest of which is background\n",
    "    Definition of SignalToBackground:  The ratio of the number of signal events within 2 sigma of the peak to the number\n",
    "    of background events in that same x-range  (this definition is just a choice)\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "     SignalToBackground : float\n",
    "     Ratio of the number of signal events within 2 sigma of the peak to the number\n",
    "    of background events in that same x-range  (this definition is just a choice) \n",
    "      \n",
    "    nSigEvents : int\n",
    "      total number of signal events generated (NOT the number in +- 2 sigma)\n",
    "      \n",
    "    seed : int\n",
    "      seed for the random number generator\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    data : array\n",
    "      the measurements of x\n",
    "    \"\"\"\n",
    "    fracOutsideTwoSigma = 4.55e-2\n",
    "    # 1-fracOutsideTwoSigma is the fraction of the Signal Events within +-2 sigma\n",
    "    # To get the total number of background events, find the number in +-2 sigma\n",
    "    # which is 4 units of x and then since the background is flat from 0 to 20\n",
    "    # multiply by 20/4=5\n",
    "    \n",
    "    nBackground = 5*nSigEvents*(1-fracOutsideTwoSigma)/SignalToBackground\n",
    "    nEvents = nSigEvents+nBackground\n",
    "    fSig = nSigEvents/nEvents\n",
    "    \n",
    "    # Make an array to hold the data (ie the x measurements)\n",
    "    data = []\n",
    "    \n",
    "    # set the random seed.  This will allow us to reproduce the results if we run again\n",
    "    np.random.seed(seed)\n",
    " \n",
    "    # Retrieve nEvents random numbers that will be used to pick which events are signal\n",
    "    # and which are background\n",
    "    n = int(nEvents)\n",
    "    tests = np.random.uniform(0,1,n)\n",
    "    bck = np.random.uniform(0,20,n)\n",
    "    sig = np.random.normal(10,1,n)\n",
    "    \n",
    "    count = 0\n",
    "    for test in tests:\n",
    "        if(test<fSig):\n",
    "            data.append(sig[count])\n",
    "        else:\n",
    "            data.append(bck[count])\n",
    "        count+=1\n",
    "    \n",
    "    # Loop over the events and pick either signal or background and draw from the appropriate \n",
    "    # pdf in each case\n",
    "    \n",
    "    # return the data to the user\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or alternatively using ROOT (do not execute both cells, just either this or the one above depending on what you're planning to use and delete the other from your notebook to avoid mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "outputs": [],
   "source": [
    "import ROOT\n",
    "\n",
    "def makeData( SignalToBackground, nSigEvents, seed=12345):\n",
    "    \"\"\"Generates a dataset consisting of nEvents some of which are signal (a Gaussian with a width of 1 centered\n",
    "    at x=5) and the rest of which is background\n",
    "    Definition of SignalToBackground:  The ratio of the number of signal events within 2 sigma of the peak to the number\n",
    "    of background events in that same x-range  (this definition is just a choice)\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "     SignalToBackground : float\n",
    "     Ratio of the number of signal events within 2 sigma of the peak to the number\n",
    "    of background events in that same x-range  (this definition is just a choice) \n",
    "      \n",
    "    nSigEvents : int\n",
    "      total number of signal events generated (NOT the number in +- 2 sigma)\n",
    "      \n",
    "    seed : int\n",
    "      seed for the random number generator\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    data : array\n",
    "      the measurements of x\n",
    "    \"\"\"\n",
    "    fracOutsideTwoSigma = 4.55e-2\n",
    "    # 1-fracOutsideTwoSigma is the fraction of the Signal Events within +-2 sigma\n",
    "    # To get the total number of background events, find the number in +-2 sigma\n",
    "    # which is 4 units of x and then since the background is flat from 0 to 20\n",
    "    # multiply by 20/4=5\n",
    "    \n",
    "    nBackground = 5*nSigEvents*(1-fracOutsideTwoSigma)/SignalToBackground\n",
    "    nEvents = nSigEvents+nBackground\n",
    "    fSig = nSigEvents/nEvents\n",
    "    \n",
    "    # Make an array to hold the data (ie the x measurements)\n",
    "    data = []\n",
    "    \n",
    "    # set the random seed.  This will allow us to reproduce the results if we run again\n",
    "    ROOT.gRandom.SetSeed(seed)\n",
    " \n",
    "    # Retrieve nEvents random numbers that will be used to pick which events are signal\n",
    "    # and which are background\n",
    "    n = int(nEvents)\n",
    "    tests = []\n",
    "    bck = []\n",
    "    sig = []\n",
    "    for i in range(n):\n",
    "        tests.append(ROOT.gRandom.Rndm())\n",
    "        bck.append(ROOT.gRandom.Rndm()*20.0)\n",
    "        sig.append(ROOT.gRandom.Gaus(10, 1))\n",
    "    \n",
    "    # Loop over the events and pick either signal or background and draw from the appropriate \n",
    "    # pdf in each case\n",
    "    count = 0\n",
    "    for test in tests:\n",
    "        if(test<fSig):\n",
    "            data.append(sig[count])\n",
    "        else:\n",
    "            data.append(bck[count])\n",
    "        count+=1\n",
    "        \n",
    "    # return the data to the user\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Below is a simple test to show you how to use this code.  Remember if you run this code multiple times you should change the seed each time (for example, you could increment the seed each time you call the function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total data events:  115\n",
      "data:  [11.047401505881462, 11.57102936217666, 10.430661187946646, 0.05376129148641384, 19.7669083856564, 18.106831513232198, 4.15271722392649, 5.849788255848496, 10.40020306144967, 18.038227453213413, 19.672617698234465, 5.150841283081662, 11.287180858495633, 10.579689779144333, 7.887401079055496, 14.621460716891141, 3.221380288584297, 12.013971356671798, 17.317289166065294, 19.67043218407111, 1.5873158075603144, 8.566945494018984, 4.0908571909285545, 9.012729810374696, 10.95527145257708, 1.8665342073964153, 5.937215509613589, 18.55168480304295, 11.380074628603907, 9.148239950472238, 8.762646787312146, 14.837243036840746, 0.9715806568853758, 14.17394790885492, 16.784866956101673, 3.3187576841390776, 15.619958759999147, 5.730732334582038, 6.129395066591146, 13.30522930699366, 2.227843432154315, 9.800887617242747, 17.757135853524453, 13.926225364708127, 8.806557533308181, 8.764287687544494, 15.30192190478613, 11.312840024517657, 1.6980832638363519, 11.653421757228093, 8.53488377794505, 6.7413276689521995, 18.55153159150779, 15.014340006863359, 11.48127650299929, 15.032879775858367, 1.5829792147641042, 17.18778151361223, 16.43008226401116, 18.197433192204176, 2.5726239501876003, 1.6356017418797042, 2.7683114556317956, 7.9875742020560025, 8.486137221619838, 9.416463283292343, 2.444870992650101, 4.027990027627418, 16.232886965680432, 9.359751481134989, 11.218985862132426, 0.14852757089226065, 11.03185451964811, 18.638642961533563, 11.408369146076902, 4.121914548905483, 14.355151245591152, 9.330381400130241, 11.264625286968007, 0.5863944578754254, 12.718007187027123, 0.64395869875848, 14.89561310283532, 9.458260044769101, 2.4350871094610738, 10.85271851577012, 1.335488864500094, 9.317461845796695, 19.92172654735651, 15.387946741479245, 11.475482273176489, 2.052705184215917, 13.996681495458294, 13.223357346558622, 0.9819426124555264, 15.845986036891812, 8.076284270302875, 8.517353883998382, 15.76374347202931, 8.231384464176744, 9.620525510103072, 3.6325768534764924, 10.765054845997017, 9.17101116623899, 3.738074978534307, 8.345821218066883, 19.78069014790599, 4.731996234110387, 18.33664665877658, 18.36794935611266, 1.8259268440640364, 9.27305449771032, 11.175910771834607, 6.273379001042549, 0.9467907447547397]\n"
     ]
    }
   ],
   "source": [
    "mydata = []\n",
    "#  make 10 signal events with a signalToBackground of 1 using random seed 123\n",
    "mydata=makeData(1,20,123)\n",
    "print(\"number of total data events: \",len(mydata))\n",
    "print(\"data: \",mydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Generate a sample of 1000 signal events with signalToBackground=0.5 and make a histogram of your results. Make sure that the number of bins in your histogram is small enough that you have at least 10 entries per bin (so that it is reasonable to do a binned fit to the histogram). To make life a bit easier, here is function you can use to make your histograms.  You are of course free to write your own function and not use this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "outputs": [],
   "source": [
    "#Import the pyplot module of matplotlib as \"plt\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Makes a histogram filled with the random numbers we generate\n",
    "def plot_histogram(samples,xtitle,ytitle, title, nbins, limits):\n",
    "   \n",
    "    #Plot the histogram of the sampled data with nbins and a nice color\n",
    "    n, bins, patches =plt.hist(samples, bins=nbins, range=limits, color=(0,0.7,0.9))  #Set the color using (r,g,b) values or\n",
    "                                                                  #  use a built-in matplotlib color\"\"\" \n",
    "    bincenters = 0.5*(bins[1:]+bins[:-1])\n",
    "    errs = np.sqrt(n)\n",
    "\n",
    "    plt.errorbar(bincenters, n, yerr=errs, fmt='none')\n",
    "    #Add some axis labels and a descriptive title\n",
    "    plt.xlabel(xtitle)\n",
    "    plt.ylabel(ytitle)\n",
    "\n",
    "    #Get rid of the extra white space on the left/right edges (you can delete these two lines without a problem)\n",
    "    xmin, xmax, ymin, ymax = plt.axis()\n",
    "    plt.axis([limits[0],limits[1],ymin,ymax])\n",
    "\n",
    "    #Not necessarily needed in a Jupyter notebook, but it doesn't hurt\n",
    "    plt.show()\n",
    "    return n, bins, patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or alternatively using ROOT as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "outputs": [],
   "source": [
    "#Import the pyplot module of matplotlib as \"plt\"\n",
    "import math\n",
    "\n",
    "\n",
    "#Makes a histogram filled with the random numbers we generate\n",
    "def plot_histogram(samples,xtitle,ytitle, title, nbins, limits):\n",
    "       \n",
    "    #Create the histogram to contain the sampled data with nbins\n",
    "    h_sampled = ROOT.TH1F(\"h_sampled\", \"Sampled data\", nbins, limits[0], limits[1])\n",
    "    #Add some axis labels and a descriptive title\n",
    "    h_sampled.SetXTitle(xtitle)\n",
    "    h_sampled.SetYTitle(ytitle)\n",
    "    #Set a nice color\n",
    "    h_sampled.SetLineColor(ROOT.kBlack)\n",
    "    \n",
    "    #Add data to the histogram\n",
    "    for s in samples:\n",
    "        h_sampled.Fill(s)\n",
    "    \n",
    "    #Draw the histogram\n",
    "    c_sampled = ROOT.TCanvas(\"c_sampled\")\n",
    "    h_sampled.Draw(\"HIST E\") #Draw histogram with error bars following Gaussian statistics\n",
    "    \n",
    "    #Adjust y-axis range\n",
    "    h_sampled.GetYaxis().SetRangeUser(0, h_sampled.GetMaximum()*1.2)\n",
    "    \n",
    "    c_sampled.Draw()\n",
    "    \n",
    "    #return the canvas and histograms if more work is needed\n",
    "    return c_sampled, h_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Pretend this is real data.  Your goal is to find the best estimate of how many events are in a Gaussian peak with unknown mean and sigma and what the uncertainty on this estimate is.  In your fit, you can make the assumption that the background is flat (a zeroth order polynomial) but that you don't have a prediction for the background rate.  Use your favorite minimization package to fit the data.  Deterimine the best estimate of the number of events in signal and the uncertainty on that estimate.  (remember, that you must let the position and width of the Gaussian and the size of the background vary in your fit).\n",
    "\n",
    "Hint: For examples of how to perform a least squared fit of a function to data see:\n",
    "- https://github.com/berkeley-physics/Python-Tutorials/blob/master/3%20-%20Specific%20topics/Fitting.ipynb\n",
    "- https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n",
    "- https://root.cern/doc/master/multifit_8py.html (ROOT, python)\n",
    "- https://root.cern/doc/master/FittingDemo_8C.html (ROOT, more complete example in C++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "What is the $\\chi^2$ per degree of freedom for your fit?  What does this number tell you about how well your fit describes the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "If a signal of size $N_S^{meas}$ has a fitted uncertainty $\\sigma_S$, the signficance of the measurement is defined to be\n",
    "$$\n",
    "S^{meas} \\equiv \\frac{N_S^{meas}}{\\sigma_S}\n",
    "$$\n",
    "When the size of the data sample is large enough that Gaussian uncertainties are appropriate, a rule of thumb can be used to give a crude estimate of the expected significance $S^{expected}$ of the measurement. This predicted signficance depends on the number of signal events ($N_{S}$) and is the number of background events populating the region **beneath the signal** ($N_B$):\n",
    "$$\n",
    "S^{expected}  \\approx  \\frac{N_S}{N_S+N_B}\n",
    "$$\n",
    "\n",
    "Repeat the above exercise changing both the number of events in your signal and the signalToBackground ratio.  Plot the values of the measured significance  $S^{meas}$ \n",
    "obtained from your fits as a function of $\\frac{N_S}{\\sqrt{N_S+N_B}}$.  How do your results compare to the simple rule of thumb quoted above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Likelihood Fits, Statistical Methods (NOT FOR THIS WEEK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this question you will:\n",
    "\n",
    "- Construct a likelihood function for a probability distribution with one free parameter\n",
    "- Determine the best fit value of the parameter and estimate its uncertainty both by graphing the likelihood function and using a standard minimization package\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Consider the problem of determining the lifetime of a species of particle that we can stop in our detector by observing its decays. Assume each time a particle stops, we set the stopping time to be $t=0$ and that we only observe decays that occur up to a time $T_{max}$ after the particle stops.  The distribution of measured times therefore follows the form:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "R(t) & =  R_0 e^{-\\Gamma t} & \\qquad \\qquad 0 \\le t \\le T_{max} \\\\\n",
    "     & =  0 &\\qquad \\qquad {\\rm otherwise}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For this problem, we'll take as the true decay parameter $\\Gamma=2\\ \\mathrm{sec}^{-1}$ and maximum time that we wait for a decay to be $T_{max}=3$ sec. We can imagine doing the experiment over many times (each experiment takes three seconds) to accumulate a lot of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "First, let's generate some fake data. Generate 1000 decay times that follow the formula above.  (Hint:  use numpy.random.exponential or ROOT.gRandom.Exp and reject events with decay times larger than $T_{max}$.  Verify that your event generation looks reasonable by making a histogram of the decay times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "outputs": [],
   "source": [
    "#Write import math\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def makeData( Gamma, Tmax, nDecays ):\n",
    "    \"\"\"Generates a dataset of decay times.  The distribution of events follows an exponential with \n",
    "    decay parameter Gamma, but where the decays are cut-off after time Tmax\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    Gamma : float\n",
    "      decay parameter of the exponential distribution\n",
    "      \n",
    "    Tmax : float\n",
    "      maximum decay time that can be generated in the dataset\n",
    "      \n",
    "    nDecays : int\n",
    "      number of decay times to generate\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    decayTimes : array\n",
    "      nDecays number of decay times generated according to the exponential distribution with decay\n",
    "      parameter gamma and maximum decay time Tmax\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make an array to hold the decay times\n",
    "    decayTimes = [0.0 for i in range(nDecays)]\n",
    "    \n",
    "    '''Your code here'''\n",
    "    \n",
    "    return decayTimes\n",
    "\n",
    "# Run your function and put plotting code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or using ROOT as follows (reminder: do not execute both cells above, just either this or the one above depending on what you're planning to use and delete the other from your notebook to avoid mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "outputs": [],
   "source": [
    "#Write import math\n",
    "import ROOT\n",
    "\n",
    "def makeData( Gamma, Tmax, nDecays ):\n",
    "    \"\"\"Generates a dataset of decay times.  The distribution of events follows an exponential with \n",
    "    decay parameter Gamma, but where the decays are cut-off after time Tmax\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    Gamma : float\n",
    "      decay parameter of the exponential distribution\n",
    "      \n",
    "    Tmax : float\n",
    "      maximum decay time that can be generated in the dataset\n",
    "      \n",
    "    nDecays : int\n",
    "      number of decay times to generate\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    decayTimes : array\n",
    "      nDecays number of decay times generated according to the exponential distribution with decay\n",
    "      parameter gamma and maximum decay time Tmax\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make an array to hold the decay times\n",
    "    decayTimes = [0.0 for i in range(nDecays)]\n",
    "    \n",
    "    '''Your code here'''\n",
    "    \n",
    "    return decayTimes\n",
    "\n",
    "# Run your function and put plotting code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Calculate the negative log-likelihood function, $-\\ln {\\cal L}$, for your data.  Express your likelihood in terms of the free parameters, $\\Gamma$.\n",
    "\n",
    "Hint: your pdf for the expected distribution of decay times $f(t) $ is an exponental that only extends to $t_{max}$ so:\n",
    "$$\n",
    "\\int_0^{t_{max}}f(t)dt = \\int_0^{t_{max}} R_0 e^{-\\Gamma t} dt = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "We will study the simulated data, pretending that we dont know what value of $\\Gamma$ was used to generate it.  We want to find the best estimate of $\\Gamma$ from the data. \n",
    "\n",
    "We saw in class that for high statistics $-2\\ln {\\cal L}$ is  distributed like a $\\chi^2$ distribution and the uncertainty on the estimate of a parameter of the function can be obtained by finding how much the you can change the parameter to increase $-2\\ln {\\cal L}$ by 1. \n",
    "Write code to calculate the negative log-likelihood:\n",
    "$$\n",
    "- 2 \\ln {\\cal L} = -2 \\sum_i \\ln f(\\Gamma, t_i)\n",
    "$$\n",
    "where the $t_i$ are the time values you generated.  Using this code, find the value of $-2\\ln {\\cal L}$ for $\\Gamma - \\Gamma_{true}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "outputs": [],
   "source": [
    "def minusloglikelihoodFn(Gamma, maxT, decayTimes):\n",
    "    \"\"\"calculates the -ln(Likelihood) for the decayTimes for specified values of maxT and Gamma\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    Gamma : float\n",
    "      hypothesis for lifetime\n",
    "      \n",
    "    Tmax : float\n",
    "      maximum time for observation\n",
    "      \n",
    "    nDecays : array\n",
    "      a dataset of decay times generated according to the distribution described above\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    minusLogLikelihood : float\n",
    "      -ln(Likelihood) given the hypothesized Gamma and maxT for the input data\n",
    "    \"\"\"  \n",
    "    minusLogLikelihood = 0.0\n",
    "    \n",
    "    '''Your code here'''\n",
    "\n",
    "    return minusLogLikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "There are lots of algorithms for finding the  minimum of a non-linear function such as our negative log-likelihood,  but we won't bother to use any of these algorithms for yet.  Instead, we will explore the minimum by inspecting the behavour of the function. Plot the value of $-\\ln {\\cal L}$ you obtain from your simulated data as you vary $\\Gamma$ in the region  of the true answer ($\\Gamma=2$).  How close is the $\\Gamma $ that gives minimum negative log-likelihood  to the true value of $\\Gamma$?  Estimate the uncertainty on your estimate of $\\Gamma$ by finding the values corresponding to an increase of ${\\cal L}$ of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "outputs": [],
   "source": [
    "ngrid=100 # How many points to scan\n",
    "G = np.arange(1.75,2.25,0.5/ngrid)  #The gamma values to scan\n",
    "LLG = np.zeros(ngrid)               #The likelihood for these values\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or using ROOT as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "outputs": [],
   "source": [
    "ngrid=100 # How many points to scan\n",
    "G_min = 1.75\n",
    "G_max = 2.25\n",
    "G_step = (G_max-G_min)/ngrid\n",
    "\n",
    "G = [] #The gamma values to scan\n",
    "LLG = [0.0 for i in range(ngrid)] #The likelihood for these values\n",
    "for i in range(ngrid):\n",
    "    G.append(G_min + G_step*i) \n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5e. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "To verify your estimate of the uncertainty on the measured value of $\\Gamma$, generate 100 samples each with 1000 events.  Histogram the estimated $\\Gamma$ for these samples and find the rms of the \"measurements.\"  How does this rms compare to your answers above?\n",
    "\n",
    "Note: while minimizing via a scan as above is instructive, here you can also try to use existing minimizers, e.g. scipy.optimize.minimize using the 'BFGS' method or ROOT.Math.Minimizer with e.g. Minuit2  (or just your custom code you wrote above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
